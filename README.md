# Проект ETL Daily Data Flow

Проект ETL Daily Data Flow предназначен для автоматизации ежедневной обработки данных, поступающих из банка. Каждый день банк предоставляет обновленные данные по паспортам, терминалам и транзакциям, которые размещаются в папке `incoming_files`. Скрипт `main.py` обрабатывает эти файлы, выполняя инкрементальную загрузку данных и поддерживая историчность с помощью методов **SCD1** и **SCD2**.

## Основные этапы процесса:

1. **Извлечение даты из имени файла**:
   - Скрипт извлекает дату из имени каждого входящего файла, чтобы использовать её для формирования исторических таблиц.

2. **Загрузка данных в staging-таблицы**:
   - Данные из файлов форматов **TXT** и **XLSX** загружаются во временные staging-таблицы в базе данных **PostgreSQL** с использованием библиотек `pandas` и `SQLAlchemy`.

3. **Обновление исторических таблиц**:
   - **SCD1 (Slowly Changing Dimension Type 1)**:
     - Для сущностей, где важна только актуальная информация, применяется метод SCD1, при котором предыдущие данные перезаписываются новыми значениями без сохранения истории изменений.
   - **SCD2 (Slowly Changing Dimension Type 2)**:
     - Для сущностей, где необходимо сохранять историю изменений (например, для таблиц `terminals` и `passport_blacklist`), используется метод SCD2. В этом случае при изменении данных создается новая запись с указанием периода её актуальности, что позволяет отслеживать изменения во времени.

4. **Генерация отчета по мошенническим операциям**:
   - На основе обработанных данных формируется отчет, позволяющий выявлять и анализировать подозрительные транзакции.

5. **Архивация обработанных файлов**:
   - После успешной обработки входящие файлы перемещаются в папку `archive`, чтобы избежать их повторной обработки в будущем.

Этот процесс обеспечивает надежную и эффективную инкрементальную загрузку данных, поддерживая актуальность информации и сохраняя историю изменений для критически важных сущностей.

# Структура проекта

```bash
├── archive/              # Каталог для архивирования обработанных файлов
├── incoming_files/       # Каталог с входящими файлами для обработки
├── py_scripts/           # Модули для создания и обновления таблиц (исторических, staging, отчётов)
│   ├── fraud_table.py    # Скрипт для создания и обновления отчёта по мошенничеству
│   ├── hist_tables.py    # Скрипты для работы с историческими таблицами
│   └── stg_tables.py     # Скрипты для создания временных (staging) таблиц
├── sql_scripts/          # SQL-скрипты для создания схем, таблиц и DML-операций
│   └── ddl_dml.sql       # Скрипт с DDL и DML операциями (для создания модели существующей у банка БД)
├── config.json           # Файл с параметрами подключения к базе данных PostgreSQL
├── main.py               # Основной скрипт, управляющий процессом ETL
├── requirements.txt      # Файл с зависимостями проекта
└── .gitignore            # Файл для исключения файлов из контроля версий
```

# Установка и запуск

## Клонирование репозитория

```bash
git clone https://github.com/stanexd/etl-daily-data-flow.git
cd etl-daily-data-flow
```

## Установка зависимостей

Убедитесь, что у вас установлен Python (рекомендуется Python 3.8+). Установите зависимости из файла `requirements.txt`:

```bash
pip install -r requirements.txt
```

## Настройка подключения к базе данных

Отредактируйте файл `config.json`, указав корректные параметры подключения к вашей базе данных PostgreSQL:

```json
{
    "host": "localhost",
    "port": "5432",
    "dbname": "your_db_name",
    "user": "your_username",
    "password": "your_password"
}
```

## Подготовка входящих файлов

Перед запуском скрипта поместите в каталог `incoming_files` три входящих файла с данными (например, файлы с транзакциями, данными терминалов и списком паспортов для проверки).

## Запуск ETL-процесса

Запустите основной скрипт:

```bash
python main.py
```

После выполнения скрипта файлы из каталога `incoming_files` будут перемещены в каталог `archive`.
